---
title: "levinmisc"
output: html_document
editor_options: 
  chunk_output_type: console
---

<!-- 
Run this 'development' chunk

Store every call to library() that you need to run chunks line by line, as in a classical Rmd for analysis
-->

```{r development, include=FALSE}
library(testthat)
library(devtools)
library(usethis)
```

<!--
# Description of your package

This will fill the description of your package.
Fill and run the content of this chunk, before anything else. 

Note: when you will use other flat templates, this part will be in a separate file. Do not be surprised!
--> 

```{r description, eval=FALSE}
# Describe your package
fusen::fill_description(
  pkg = here::here(),
  fields = list(
    Title = "Miscellaneous Convenience Functions",
    Description = "A set of miscellaneous convenience functions.",
    `Authors@R` = c(
      person("Michael", "Levin", email = "mglev1n@gmail.com", role = c("aut", "cre"))
    )
  )
)
# Define License with use_*_license()
usethis::use_mit_license("Michael Levin")
```

## Introduction

The `targets` package is a tool for developing reproducible research workflows in R. Details of the package motivation and tools are described in detail: https://books.ropensci.org/targets/ and https://docs.ropensci.org/targets/.

### Example Workflow

1. Create a new R project

1. Run the `levinmisc::populate_targets_proj()` function in the R console - this function is described below in detail, and will initialize your project with helpful files/folders to start building your `targets` pipeline.

1. Modify the `Pipelines.qmd` file to specify your analyses. The `targets` documentation can be useful for specifics.

1. Knit/Render `Pipelines.qmd` to convert your markdown document into a series of R scripts that will actually run your analyses.

1. Run `submit-targets.sh` from a terminal window to submit your `targets` pipeline to the LPC for execution. Details of possible command line arguments to this script are described below.

1. Modify `Results.qmd` to present the results of your analyses. Rendering this file allows you to mix text describing your analyses/methods and include citations alongside the actual results of your pipeline. The `targets::tar_read()` function should be used heavily to load pre-computed results from your pipeline.

## populate_targets_proj()

The `populate_targets_proj` function can be run within a new project folder to initialize the project with the files/folders necessary for deploying a `targets` pipeline on the LPC at Penn. This includes creating LSF templates, a `Pipelines.qmd` file containing boilerplate for running analyses, and a `Results.qmd` file which can be used to visualize the results.

```{r function-populate_targets_proj}
#' Create a minimal targets template in the current project
#' 
#' @description
#' This function creates a minimal targets template in the current directory. This includes creating a `Pipelines.qmd` file containing boilerplate for running analyses, and a `Results.qmd` file which can be used to visualize the results. Parallelization of the pipeline is implemented using [targets::tar_make_clustermq()], using pre-filled using parameters specific to the LPC system at Penn.
#'
#' @param title (character) base name for project files (eg. "{title}-Pipeline.qmd" and "{title}-Results.qmd")
#' @param log_folder (character) directory for LSF logs
#' @param overwrite (logical) overwrite existing template files
#'
#' @export
#' @concept targets

populate_targets_proj <- function(title,
                                  log_folder = "build_logs",
                                  overwrite = FALSE) {
  # Create title
  if (missing(title)) {
    title <- basename(here::here())
  }

  # Check if files already exist
  if (file.exists(".clustermq_lsf.tmpl") | file.exists("*-Results.qmd") | file.exists("*-Pipeline.qmd")) {
    cli::cli_alert_danger("Files already exist")
    overwrite <- yesno::yesno("Overwrite existing files?")
    if (!overwrite) cli::cli_abort("Exiting")
  }

  # Create build_logs folder
  fs::dir_create(log_folder)
  cli::cli_alert_success("Build log folder created at {.file {log_folder}}")

  # Copy clustermq lsf template
  fs::file_copy(fs::path_package("extdata", ".clustermq_lsf.tmpl", package = "levinmisc"),
    ".clustermq_lsf.tmpl",
    overwrite = overwrite
  )
  cli::cli_alert_success("Clustermq template created at {.file .clustermq_lsf.tmpl}")

  # Copy batch script for clustermq
  fs::file_copy(fs::path_package("extdata", ".make-targets.sh", package = "levinmisc"),
    ".make-targets.sh",
    overwrite = overwrite
  )
  cli::cli_alert_success("Clustermq template created at {.file .make-targets.sh}")

  # Copy batch script to submit targets job
  fs::file_copy(fs::path_package("extdata", "submit-targets.sh", package = "levinmisc"),
    "submit-targets.sh",
    overwrite = overwrite
  )
  cli::cli_alert_success("Targets submission script created at {.file submit-targets.sh}")

  # Copy targets pipeline template
  fs::file_copy(fs::path_package("extdata", "Pipeline.qmd", package = "levinmisc"),
    paste0(title, "-Pipeline.qmd"),
    overwrite = overwrite
  )
  cli::cli_alert_success("Targets Pipeline template created at {.file {paste0(title, '-Pipeline.qmd')}}")

  # Copy targets results template
  fs::file_copy(fs::path_package("extdata", "Results.qmd", package = "levinmisc"),
    paste0(title, "-Results.qmd"),
    overwrite = overwrite
  )
  cli::cli_alert_success("Targets Results template created at {.file {paste0(title, '-Results.qmd')}}")
}
```

<!--
Here is an example on how to use the function.
This should be a reproducible and working example
-->

```{r examples-populate_targets_proj}
#' \dontrun{
#' populate_targets_proj("test")
#' }
```

The `populate_targets_proj()` function creates several files/folders within the project directory. `.make-targets.sh` and `.clustermq_lsf.tmpl` are hidden helper files which are not designed for user interaction, but necessary for submission of jobs to the LSF scheduler. The other files are designed to be edited/used by the user:

  - `Pipeline.qmd` - Quarto markdown file which can be used to create a Target Markdown document that specifies a `targets` pipeline for your analyses. See: https://books.ropensci.org/targets/literate-programming.html#target-markdown for details. Remember to knit/render this document in order to generate the pipeline.
  
  - `Results.qmd` - Quarto markdown file which can be used to display the results generated by the `targets` pipeline specified in `Pipeline.qmd`
  
  - `build_logs/` - Directory where jobs logs are stored
  
  - `submit-targets.sh`- This is a bash script which can be used to run your `targets` pipeline, once `Pipeline.qmd` has been knit/rendered. This script can be run directly from the submission host.
  
## submit-targets.sh
This script is used to actually submit your pipeline to the LPC once `Pipeline.qmd` has been knit/rendered. This should be run from a terminal session from the root directory of your project. The only function of this script is to submit your pipeline to the LPC for analysis, and can be run directly from a submission host (eg. `scisub7`). The script can accept command line arguments, which can be useful for parallelizing your pipeline over multiple workers/CPUs:
  
```
Usage: ./submit-targets.sh [-n NUM_WORKERS] [-j JOB_NAME] [-o OUTPUT_LOG] [-e ERROR_LOG] [-q QUEUE] [-m MEMORY] [-s SLACK] [-h HELP]

Submit a job using the LSF scheduler with the specified number of CPUs and memory usage.

Options:
  -n NUM_WORKERS Number of workers (cpu cores) to request for running the targets pipeline (default: 1)
  -j JOB_NAME    Name of the job (default: make_targets)
  -o OUTPUT_LOG  Path to the output log file (default: build_logs/targets_%J.out)
  -e ERROR_LOG   Path to the error log file (default: build_logs/targets_%J.err)
  -q QUEUE       Name of the queue to submit the job to (default: voltron_normal)
  -m MEMORY      Memory usage for the job in megabytes (default: 16000)
  -s SLACK       Enable slack notifications; requires setup using slackr::slack_setup() (default: false)
  -h HELP        Display this help message and exit
```

### Slack Notifications

Slack can be used to automatically notify the user of pipeline start/finish using the `-s true` command line flag:

```
./submit-targets.sh -s true
```

Slack notifications are provided using the `slackr` package. The package must be configured separately before Slack notifications are enabled. See https://mrkaye97.github.io/slackr/index.html for more information about `slackr` setup and generation of an Slack API token.


<!--
Here are some unit tests to verify the function works as expected.
-->

```{r tests-populate_targets_proj}
# Functions to allow creating files in new temporary directory
dir_empty <- function(x) {
  unlink(x, recursive = TRUE, force = TRUE)
  dir.create(x)
}

test_with_dir <- function(desc, ...) {
  new <- tempfile()
  dir_empty(new)
  withr::with_dir( # or local_dir()
    new = new,
    code = {
      tmp <- capture.output(
        testthat::test_that(desc = desc, ...)
      )
    }
  )
  invisible()
}

test_with_dir("populate_targets_proj creates log folder", {
  populate_targets_proj()
  expect_true(dir.exists("build_logs"))
})

test_with_dir("populate_targets_proj creates .clustermq_lsf.tmpl", {
  populate_targets_proj()
  expect_true(file.exists(".clustermq_lsf.tmpl"))
})

test_with_dir("populate_targets_proj creates .make-targets.sh", {
  populate_targets_proj()
  expect_true(file.exists(".make-targets.sh"))
})

test_with_dir("populate_targets_proj creates submit-targets.sh", {
  populate_targets_proj()
  expect_true(file.exists("submit-targets.sh"))
})

test_with_dir("populate_targets_proj creates pipeline and results .qmd templates", {
  populate_targets_proj()
  expect_true(length(fs::dir_ls(".", glob = "*-Results.qmd")) > 0)
  expect_true(length(fs::dir_ls(".", glob = "*-Pipeline.qmd")) > 0)
})
```

### Use {crew} for parallelization

By default, `populate_targets_proj()` creates a `targets` pipeline that uses the `tar_make_clustermq()` function to execute the pipeline using LPC resources. This function creates a cluster of persistent workers, and therefore is not very flexible in terms of varying the number or resources allocated to complete individual targets. More recently, the `crew` and `crew.cluster` packages have enabled the use of heterogenous workers (<https://books.ropensci.org/targets/crew.html#heterogeneous-workers>), which can be deployed either locally or on HPC resources. The `use_crew_lsf()` function is designed to return a block of code to rapidly enable the use of heterogeneous workers on the Penn LPC. By default, this function creates workers that submit to different queues (eg. `voltron_normal`, `voltron_long`), and allocate different resources (eg. a "normal" worker will use 1 core and 16GB memory, while a "long" worker will use 1 core and 10GB memory).
    
```{r function-use_crew_lsf}
#' Use Crew LSF
#' 
#' @description
#' `r lifecycle::badge("experimental")`
#' 
#' This function returns a template for using `crew.cluster` in a targets project, enabling the parallel execution of a targets workflow. By default, the template is pre-filled using parameters specific to the LPC system at Penn. By default, this function creates workers that submit to different queues (eg. `voltron_normal`, `voltron_long`), and allocate different resources (eg. a "normal" worker will use 1 core and 16GB memory, while a "long" worker will use 1 core and 10GB memory).
#'
#' @return A code block to copy/paste into a targets project
#'
#' @export
#' @concept targets

use_crew_lsf <- function() {
  title <- basename(here::here())

  command <- glue::glue(
    "library(targets)
library(tarchetypes)
library(crew)
library(crew.cluster)

controller_local <- crew_controller_local(
  name = '{title}_local',
  workers = 1,
  seconds_idle = 10
)

controller_lsf_normal <- crew.cluster::crew_controller_lsf(
  name = '{title}_normal',
  workers = 20L,
  lsf_memory_gigabytes_limit = 16,
  script_dir = tools::R_user_dir('crew.cluster', which = 'cache'),
  lsf_log_output = 'build_logs/crew-%J.log',
  lsf_log_error = 'build_logs/crew-%J.err',
  script_lines = c(
    \"#BSUB-q voltron_normal\",
    \"export R_LIBS_USER=$HOME/R/rocker-rstudio/bioconductor-tidyverse_3.17\",
    \"export OMP_NUM_THREADS=1\",
    \"export SINGULARITY_BIND='/project/:/project/, /appl/:/appl/, /lsf/:/lsf/, /scratch/:/scratch, /static:/static'\",
    \"singularity exec --pwd {getwd()} /project/voltron/rstudio/containers/bioconductor-tidyverse_3.17.sif \\\\\"
  ),
  verbose = TRUE
)

controller_lsf_long <- crew.cluster::crew_controller_lsf(
  name = '{title}_long',
  workers = 150L,
  lsf_memory_gigabytes_limit = 10,
  script_dir = tools::R_user_dir('crew.cluster', which = 'cache'),
  lsf_log_output = 'build_logs/crew-%J.log',
  lsf_log_error = 'build_logs/crew-%J.err',
  script_lines = c(
    \"#BSUB-q voltron_long\",
    \"export R_LIBS_USER=$HOME/R/rocker-rstudio/bioconductor-tidyverse_3.17\",
    \"export OMP_NUM_THREADS=1\",
    \"export SINGULARITY_BIND='/project/:/project/, /appl/:/appl/, /lsf/:/lsf/, /scratch/:/scratch, /static:/static'\",
    \"singularity exec --pwd {getwd()} /project/voltron/rstudio/containers/bioconductor-tidyverse_3.17.sif \\\\\"
  ),
  verbose = TRUE
)

controller_lsf_highmem <- crew.cluster::crew_controller_lsf(
  name = '{title}_highmem',
  workers = 5L,
  lsf_memory_gigabytes_limit = 96,
  script_dir = tools::R_user_dir('crew.cluster', which = 'cache'),
  lsf_log_output = 'build_logs/crew-%J.log',
  lsf_log_error = 'build_logs/crew-%J.err',
  script_lines = c(
    \"#BSUB-q voltron_normal\",
    \"export R_LIBS_USER=$HOME/R/rocker-rstudio/bioconductor-tidyverse_3.17\",
    \"export OMP_NUM_THREADS=1\",
    \"export SINGULARITY_BIND='/project/:/project/, /appl/:/appl/, /lsf/:/lsf/, /scratch/:/scratch, /static:/static'\",
    \"singularity exec --pwd {getwd()} /project/voltron/rstudio/containers/bioconductor-tidyverse_3.17.sif \\\\\"
  ),
  verbose = TRUE
)

controller_lsf_multicore <- crew.cluster::crew_controller_lsf(
  name = '{title}_multicore',
  workers = 5L,
  lsf_cores = 16,
  lsf_memory_gigabytes_limit = 96,
  script_dir = tools::R_user_dir('crew.cluster', which = 'cache'),
  lsf_log_output = 'build_logs/crew-%J.log',
  lsf_log_error = 'build_logs/crew-%J.err',
  script_lines = c(
    \"#BSUB-q voltron_normal\",
    \"export R_LIBS_USER=$HOME/R/rocker-rstudio/bioconductor-tidyverse_3.17\",
    \"export OMP_NUM_THREADS=16\",
    \"export SINGULARITY_BIND='/project/:/project/, /appl/:/appl/, /lsf/:/lsf/, /scratch/:/scratch, /static:/static'\",
    \"singularity exec --pwd {getwd()} /project/voltron/rstudio/containers/bioconductor-tidyverse_3.17.sif \\\\\"
  ),
  verbose = TRUE
)

# define some global options/functions common to all targets
options(tidyverse.quiet = TRUE)

tar_option_set(
  packages = c('tidyverse', 'targets', 'tarchetypes', 'arrow', 'vroom', 'data.table', 'qs', 'gt', 'crew', 'crew.cluster'),
  controller = crew::crew_controller_group(controller_lsf_long, controller_lsf_normal, controller_lsf_highmem, controller_lsf_multicore, controller_local),
  resources = tar_resources(
    crew = tar_resources_crew(controller = '{title}_normal')
  ),
  storage = 'worker',
  retrieval = 'worker',
  cue = tar_cue(format = FALSE)
)"
  )

  cli::cli_h1("Copy the below code to the `global` chunk in your targets project:")

  return(command)
}
```

  
```{r example-use_crew_lsf}
#' \dontrun{
#' use_crew_lsf()
#' }
```
  
```{r tests-use_crew_lsf}
test_that("use_crew_lsf works", {
  expect_true(inherits(use_crew_lsf, "function"))
})
```
  

<!-- 
# Inflate your package

You're one inflate from paper to box.
Build your package from this very Rmd using `fusen::inflate()` 
-->


```{r development-inflate, eval=FALSE}
# Execute in the console directly
fusen::inflate(flat_file = "dev/LPC_targets_project.Rmd", vignette_name = "Create a New Targets Pipeline for LPC")
```

<!-- 
- Verify your `"DESCRIPTION"` file has been updated
- Verify your function is in `"R/"` directory
- Verify your test is in `"tests/testthat/"` directory
- Verify this Rmd appears in `"vignettes/"` directory 
-->
